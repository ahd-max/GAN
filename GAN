# -*- coding: utf-8 -*-
"""
Created on Tue Nov 21 13:48:22 2023

@author: ahd10
"""
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import os
import sys
import random
import numpy as np
import pandas as pd
from pandas import DataFrame
import math
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import scipy.io 

# 生成器模型
def build_generator(latent_dim):
    model = tf.keras.Sequential()
    model.add(layers.Dense(8*8*256, input_dim=latent_dim))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.Reshape((8, 8, 256)))
    model.add(layers.Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding='same'))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2DTranspose(32, kernel_size=5, strides=2, padding='same'))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding='same', 
                                     activation='linear'))
    model.add(layers.Conv2DTranspose(1, kernel_size=5, padding='same', 
                                     activation='sigmoid'))
    return model

# 判别器模型
def build_discriminator(input_shape):
    model = tf.keras.Sequential()
    
    model.add(layers.Conv2D(32, kernel_size=5, strides=2, input_shape=input_shape, padding='same'))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization())
    # model.add(layers.Flatten())
    model.add(layers.Conv2D(64, kernel_size=5, strides=2, padding='same'))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dropout(0.3))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2D(128, kernel_size=5, strides=2, padding='same'))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dropout(0.3))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2D(256, kernel_size=5, strides=2, padding='same'))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dropout(0.3))
    model.add(layers.BatchNormalization())
    model.add(layers.Flatten())
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

# 生成对抗网络模型
def build_gan(generator, discriminator):
    discriminator.trainable = False
    model = tf.keras.Sequential()
    model.add(generator)
    model.add(discriminator)
    return model


mat_file1=r'D:\data\random_matricesN.mat'
data= scipy.io.loadmat(mat_file1)
sample=data['time_series_data']

# 定义参数
latent_dim = 100
input_shape = (128, 128, 1)
batch_size = 32
epochs = 10000

# 构建和编译判别器
discriminator = build_discriminator(input_shape)
discriminator.compile(loss='binary_crossentropy',
                      optimizer=keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.99),
                      metrics=['accuracy'])

# 构建生成器
generator = build_generator(latent_dim)

# 构建生成对抗网络
gan = build_gan(generator, discriminator)
gan.compile(loss='binary_crossentropy', 
            optimizer=keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.9))

generator.summary()
discriminator.summary()



# 训练GAN并绘制损失图
def train_gan(generator, discriminator, gan, data, epochs, batch_size):
    d_losses = []  # 记录判别器损失
    g_losses = []  # 记录生成器损失
    d_loss = None
    g_loss= None
    for epoch in range(epochs):
        for _ in range(len(sample) // batch_size):
            noise = np.random.normal(0, 1, (batch_size, latent_dim))
            generated_images = generator.predict(noise)

            idx = np.random.randint(0, sample.shape[0], batch_size)
            real_images = sample[idx]
            real_images = np.reshape(real_images, (batch_size, 128,128,1))


            labels_real = np.ones((batch_size, 1))
            labels_fake = np.zeros((batch_size, 1))
            
            d_loss_real = discriminator.train_on_batch(real_images, labels_real)
            d_loss_fake = discriminator.train_on_batch(generated_images, labels_fake)

            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

            noise = np.random.normal(0, 1, (batch_size, latent_dim))
            labels_gan = np.ones((batch_size, 1))

            g_loss = gan.train_on_batch(noise, labels_gan)
            # 记录损失
        d_losses.append(d_loss)
        g_losses.append(g_loss)
            
    return d_losses, g_losses

            #print(f"Epoch {epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]")

d_losses, g_losses = train_gan(generator, discriminator, gan, data, epochs, batch_size)

# 训练GAN并绘制损失图

# 绘制损失曲线
plt.plot(range(epochs), d_losses, label='Discriminator Loss')
plt.plot(range(epochs), g_losses, label='Generator Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# 调用训练函数
train_gan(generator, discriminator, gan, data, epochs, batch_size)


noise_for_generation = np.random.rand(1, 100)
generated_image = generator.predict(noise_for_generation)

generated_imagedata=generated_image[0]

result_array = np.concatenate(generated_imagedata, axis=1)

# # r=sample[0]
# plt.imshow(result_array, cmap='viridis')  # cmap是颜色映射，可以根据需要更改
# #plt.imshow(r, cmap='viridis')  # cmap是颜色映射，可以根据需要更改
# plt.colorbar()  # 显示颜色条
# plt.title('Matrix as Image')
# plt.show()
